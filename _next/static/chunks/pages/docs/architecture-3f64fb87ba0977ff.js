(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[163],{2262:function(e,t,a){(window.__NEXT_P=window.__NEXT_P||[]).push(["/docs/architecture",function(){return a(2803)}])},8177:function(e,t,a){"use strict";var i=a(5893);t.Z={logo:(0,i.jsx)("svg",{xmlns:"http://www.w3.org/2000/svg",width:"130",height:"30.555",viewBox:"0 0 130 30.555",children:(0,i.jsx)("g",{id:"Group_1","data-name":"Group 1",transform:"translate(-121.7 -308.1)",children:(0,i.jsx)("path",{id:"Union_1","data-name":"Union 1",d:"M95.7,15.435C95.7,6.4,102.979,0,113.224,0c8.6,0,15.028,4.124,16.776,10.722l-9.673,2c-.785-3.3-3.213-5.067-7.1-5.067-4.711,0-7.6,2.946-7.6,7.619,0,4.713,2.855,7.58,7.638,7.58,3.855,0,6.389-1.767,7.068-5.066l9.6,2.357c-2.106,6.6-8.353,10.408-17.026,10.408C102.408,30.555,95.7,24.664,95.7,15.435ZM73.959,29.613,61.5.982H71.889l6.818,17.6a13.878,13.878,0,0,1,.856,3.064h.143a19.312,19.312,0,0,1,.857-3.064L87.381.982H97.768L85.31,29.613Zm-43.3,0V13.9a41.259,41.259,0,0,1,.5-6.048h-.179c-.214,1.021-1.071,3.77-1.749,5.695l-5.64,16.063H14.885L9.566,13.628c-.714-2.239-1.321-4.4-1.642-5.773H7.781A37.945,37.945,0,0,1,8.138,13.9v15.71H0V.982H13.885l4.64,14.531a16.675,16.675,0,0,1,.964,3.574h.072a21.442,21.442,0,0,1,.964-3.653L24.986.982H39.228V29.613Zm11.458-.039V.982h9.388v20.7H67.891v7.894Z",transform:"translate(121.7 308.1)",fill:"#fff"})})}),project:{link:"https://github.com/mlVibeCaptions"},footer:{text:"MLVC 2023 - This website is an online documentation of Antonis kalagkatsis' MSc thesis in the National and Kapodistrian University of Athens, Department of Communication and Media Studies, Digital Communication Media and Interaction Environments"},primaryHue:189,feedback:{content:null},editLink:{text:null},sidebar:{defaultMenuCollapseLevel:1},useNextSeoProps:()=>({titleTemplate:"%s â€¢ MLVC"})}},2803:function(e,t,a){"use strict";a.r(t),a.d(t,{default:function(){return u.Z}});var i=a(5893),n=a(4526),o=a(7928),r=a(8177);a(5513);var s=a(1151),c=a(5675),d=a.n(c),l={src:"/_next/static/media/vgg-1.1b43e5fa.png",height:1988,width:1329,blurDataURL:"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUAAAAICAYAAAAx8TU7AAAAs0lEQVR4nAGoAFf/ATExMYH49/c6ISYs9enl4Av8/PrJASwrK8IYGxs9BAQGAOfl4wAEBATGAQMMDbdqXlxI6+vr+cfMzQcWFBS7AQAMGrlpVT9G8PL0+8jR2gUUEA29ATc6NrkYHh5G7Obr+/Tw7wUDBQW9AVxZWrgIBgdHsbey+h8eIQb+/v68ATw1N78QDxBA9QD+APDt7AABAQHDASotLJcCAwNDJCMq8+Hf2Q3+/f2/eXRHTMzseOUAAAAASUVORK5CYII=",blurWidth:5,blurHeight:8},h={src:"/_next/static/media/vgg-2.662a0c8a.png",height:3840,width:1920,blurDataURL:"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAQAAAAICAYAAADeM14FAAAAgElEQVR42h3NawqCQBhG4dlMOGraXJkKLEFBbTTtStR2Gr5lv5n/n8NhacKpOJRw2z2ybP1lVduQnyakSQJtTGDFsaSuH7DJcyilAjPWUlXXkFJCCBHY0Ht63G/gcQzzT8ZzQ+/XiIinsEbP4nKi5+eKaMVh7Sys09T6Ds7tlu0PLAcubja8wV4AAAAASUVORK5CYII=",blurWidth:4,blurHeight:8},u=a(6092);function p(e){let t=Object.assign({h1:"h1",h2:"h2",p:"p"},(0,s.ah)(),e.components);return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.h1,{children:"Architecture"}),"\n",(0,i.jsx)(t.h2,{id:"cnn001",children:"CNN001"}),"\n",(0,i.jsx)(t.p,{children:"The methodology I followed to develop the CNN is based on the complexity simplification abstraction process just as I did in the definition of the ACLF problem in the theoretical part. Consequently, the first implementation of the CNN which for consistency I call CNNNetwork, is based on the VGGNet architecture.\nCNNNetwork."}),"\n",(0,i.jsx)(t.p,{children:"The first version of this implementation has a fixed size of input variables. The input to the network is a Tensor with three dimensions expressed as Tensor(1, 64, 44). Each audio file is transformed to a duration of one second, and the number of features entering the network for each file of that duration is 2560. The spectrogram image represented by the above Tensor is input through a stack of convolutional layers. This stack consists of four convolutional layers (CONV). Each CONV has a kernel dimension (receptive field) of a 3 x 3 matrix and the convolution step (stride) has the numerical value 1. After each convolutional layer the nonlinearity function of the ReLU model is followed. Then spatial pooling is performed at the end of each CONV using Max Pooling. Max Pooling is performed with a kernel dimension of 2 x 2 with the convolution step (stride) having the numerical value 2. Following the transactional stack is a Fully-Connected (FC) layer which has a channel count of 5,122. The last layer in this architecture is a soft-max layer which becomes the ACLF and classifies the audio files into two classes."}),"\n",(0,i.jsx)(d(),{src:l,width:900,height:400}),"\n",(0,i.jsx)(t.h2,{id:"cnn002",children:"CNN002"}),"\n",(0,i.jsx)(t.p,{children:"The next architectural approach also focuses on the VGG architecture but this time with the implementation of a much deeper model. I will refer to this model as VGGish for consistency. The architectural approach of the VGGish model is based on a publication entitled CNN Architectures for Large-Scale Audio Classification in which they address an Acoustic Event Detection problem. This architecture was created both to test the data on a network with much greater depth, and also to allow the duration of the file to be realized and hence the number of features to be adaptive when they enter the ACLF.\nVGGish consists of a sequence of six convolutional layers and a sequence of four layers of linear transformations, and each of these is followed by a ReLU nonlinearity function. Each convolutional layer has a kernel dimension (receptive field) of a 3 x 3 matrix with the convolution step (stride) set to the numerical value of 1 just like in CNNNetwork. Max Pooling layers with a kernel dimension of 2 x 2 and the convolution step (stride) set to the numerical value of 2 have been placed in the 1st, 2nd, 4th layers, while an Adaptive Max Pooling layer has been placed in the last one. Then 3 FC layers are placed where each of them is followed by a ReLU nonlinearity function layer"}),"\n",(0,i.jsx)(d(),{src:h,width:900,height:400})]})}e=a.hmd(e),(0,n.j)({pageNextRoute:"/docs/architecture",pageOpts:{filePath:"pages/docs/architecture.mdx",route:"/docs/architecture",frontMatter:{},pageMap:[{kind:"Meta",data:{index:{title:"Home",type:"page",display:"hidden",theme:{typesetting:"article",layout:"full",breadcrumb:!0,footer:!0,sidebar:!1,toc:!0,pagination:!1}},docs:{title:"Research",type:"page"},dataset:{title:"Dataset",type:"page",theme:{typesetting:"article",layout:"default",breadcrumb:!0,footer:!0,sidebar:!0,toc:!0,pagination:!0}},experiments:{title:"Experiments",type:"page"},technology:{title:"Technology",type:"menu",items:{pytorch:{title:"Pytorch",href:"https://pytorch.org/",newWindow:!0},torchaudio:{title:"torch audio",href:"https://pytorch.org/audio/stable/index.html",newWindow:!0}}}}},{kind:"MdxPage",name:"dataset",route:"/dataset"},{kind:"Folder",name:"docs",route:"/docs",children:[{kind:"Meta",data:{abstract:{title:"Abstract",theme:{typesetting:"article",layout:"full"}},"Research Objectives":{type:"separator",title:"Research Objectives"},"problem-definition":"Problem definition",limitations:"Limitations","Project Architecture":{type:"separator",title:"Project Architecture"},"dl-pipeline":"DL Pipeline","technology-stack":"Technology Stack","Collecting data":{type:"separator",title:"Data Collection"},scraping:"Scraping",dataset:"Dataset","Exploratory analysis":{type:"separator",title:"Exploratory data analysis"},"data-exploration":"Audio Analysis","feature-extraction":"Feature Extraction","pre-processing":"Pre-processing",convolutional_network:{type:"separator",title:"Model"},cnn:"Convolutional Neural Network (CNN)",architecture:"Architecture",training:"Training",performance:"Performance",Conclusion:{type:"separator",title:"Conclusions"},contribution:"Contribution",discussion:"Discussion",endnotes:{type:"separator",title:"Endnotes"},sources:"Sources",references:"References","audio-datasets":"Audio Datasets"}},{kind:"MdxPage",name:"abstract",route:"/docs/abstract"},{kind:"MdxPage",name:"architecture",route:"/docs/architecture"},{kind:"MdxPage",name:"audio-datasets",route:"/docs/audio-datasets"},{kind:"MdxPage",name:"cnn",route:"/docs/cnn"},{kind:"MdxPage",name:"contribution",route:"/docs/contribution"},{kind:"MdxPage",name:"data-exploration",route:"/docs/data-exploration"},{kind:"Folder",name:"dataset",route:"/docs/dataset",children:[{kind:"Meta",data:{"design-principles":"Design Principles",taxonomy:"Taxonomy Construction","descriptive-statistics":"Descriptive Statistics"}},{kind:"MdxPage",name:"descriptive-statistics",route:"/docs/dataset/descriptive-statistics"},{kind:"MdxPage",name:"design-principles",route:"/docs/dataset/design-principles"},{kind:"MdxPage",name:"taxonomy",route:"/docs/dataset/taxonomy"}]},{kind:"MdxPage",name:"discussion",route:"/docs/discussion"},{kind:"MdxPage",name:"dl-pipeline",route:"/docs/dl-pipeline"},{kind:"MdxPage",name:"feature-extraction",route:"/docs/feature-extraction"},{kind:"MdxPage",name:"limitations",route:"/docs/limitations"},{kind:"MdxPage",name:"performance",route:"/docs/performance"},{kind:"MdxPage",name:"pre-processing",route:"/docs/pre-processing"},{kind:"MdxPage",name:"problem-definition",route:"/docs/problem-definition"},{kind:"MdxPage",name:"references",route:"/docs/references"},{kind:"Folder",name:"scraping",route:"/docs/scraping",children:[{kind:"Meta",data:{"streaming-services":"Streaming Service APIs","data-quality":"Quality of data","store-data":"Store Data",transformation:"Transformation & Cleaning"}},{kind:"MdxPage",name:"data-quality",route:"/docs/scraping/data-quality"},{kind:"MdxPage",name:"store-data",route:"/docs/scraping/store-data"},{kind:"MdxPage",name:"streaming-services",route:"/docs/scraping/streaming-services"},{kind:"MdxPage",name:"transformation",route:"/docs/scraping/transformation"}]},{kind:"MdxPage",name:"scraping",route:"/docs/scraping"},{kind:"MdxPage",name:"sources",route:"/docs/sources"},{kind:"MdxPage",name:"technology-stack",route:"/docs/technology-stack"},{kind:"MdxPage",name:"training",route:"/docs/training"}]},{kind:"Folder",name:"experiments",route:"/experiments",children:[{kind:"Meta",data:{Experiments:{type:"separator",title:"Experiments"},archive:"Archive",binary:"Binary Classification"}},{kind:"MdxPage",name:"archive",route:"/experiments/archive"},{kind:"Folder",name:"binary",route:"/experiments/binary",children:[{kind:"Meta",data:{Training:{type:"separator",title:"Training"},"experiment-1":"Experiment 1","experiment-2":"Experiment 2","experiment-3":"Experiment 3","experiment-4":"Experiment 4","experiment-5":"Experiment 5"}},{kind:"MdxPage",name:"experiment-1",route:"/experiments/binary/experiment-1"},{kind:"MdxPage",name:"experiment-2",route:"/experiments/binary/experiment-2"},{kind:"MdxPage",name:"experiment-3",route:"/experiments/binary/experiment-3"},{kind:"MdxPage",name:"experiment-4",route:"/experiments/binary/experiment-4"},{kind:"MdxPage",name:"experiment-5",route:"/experiments/binary/experiment-5"}]},{kind:"MdxPage",name:"binary",route:"/experiments/binary"}]},{kind:"MdxPage",name:"index",route:"/"}],headings:[{depth:1,value:"Architecture",id:"architecture"},{depth:2,value:"CNN001",id:"cnn001"},{depth:2,value:"CNN002",id:"cnn002"}],timestamp:167870459e4,flexsearch:{codeblocks:!0},title:"Architecture"},nextraLayout:o.ZP,themeConfig:r.Z,Content:function(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:t}=Object.assign({},(0,s.ah)(),e.components);return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(p,{...e})}):p(e)},hot:e.hot,pageOptsChecksum:"000000097c0aa65",dynamicMetaModules:[]})}},function(e){e.O(0,[774,32,888,179],function(){return e(e.s=2262)}),_N_E=e.O()}]);