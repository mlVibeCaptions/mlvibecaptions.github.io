(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[430],{1834:function(e,t,i){(window.__NEXT_P=window.__NEXT_P||[]).push(["/docs/discussion",function(){return i(7833)}])},8177:function(e,t,i){"use strict";var n=i(5893);t.Z={logo:(0,n.jsx)("svg",{xmlns:"http://www.w3.org/2000/svg",width:"130",height:"30.555",viewBox:"0 0 130 30.555",children:(0,n.jsx)("g",{id:"Group_1","data-name":"Group 1",transform:"translate(-121.7 -308.1)",children:(0,n.jsx)("path",{id:"Union_1","data-name":"Union 1",d:"M95.7,15.435C95.7,6.4,102.979,0,113.224,0c8.6,0,15.028,4.124,16.776,10.722l-9.673,2c-.785-3.3-3.213-5.067-7.1-5.067-4.711,0-7.6,2.946-7.6,7.619,0,4.713,2.855,7.58,7.638,7.58,3.855,0,6.389-1.767,7.068-5.066l9.6,2.357c-2.106,6.6-8.353,10.408-17.026,10.408C102.408,30.555,95.7,24.664,95.7,15.435ZM73.959,29.613,61.5.982H71.889l6.818,17.6a13.878,13.878,0,0,1,.856,3.064h.143a19.312,19.312,0,0,1,.857-3.064L87.381.982H97.768L85.31,29.613Zm-43.3,0V13.9a41.259,41.259,0,0,1,.5-6.048h-.179c-.214,1.021-1.071,3.77-1.749,5.695l-5.64,16.063H14.885L9.566,13.628c-.714-2.239-1.321-4.4-1.642-5.773H7.781A37.945,37.945,0,0,1,8.138,13.9v15.71H0V.982H13.885l4.64,14.531a16.675,16.675,0,0,1,.964,3.574h.072a21.442,21.442,0,0,1,.964-3.653L24.986.982H39.228V29.613Zm11.458-.039V.982h9.388v20.7H67.891v7.894Z",transform:"translate(121.7 308.1)",fill:"#fff"})})}),project:{link:"https://github.com/mlVibeCaptions"},footer:{text:"MLVC 2023 - This website is an online documentation of Antonis kalagkatsis' MSc thesis in the National and Kapodistrian University of Athens, Department of Communication and Media Studies, Digital Communication Media and Interaction Environments"},primaryHue:189,feedback:{content:null},editLink:{text:null},sidebar:{defaultMenuCollapseLevel:1},useNextSeoProps:()=>({titleTemplate:"%s â€¢ MLVC"})}},7833:function(e,t,i){"use strict";i.r(t),i.d(t,{default:function(){return d.Z}});var n=i(5893),a=i(4526),o=i(7928),s=i(8177);i(5513);var r=i(1151);i(5675);var d=i(6092);function c(e){let t=Object.assign({h1:"h1",p:"p"},(0,r.ah)(),e.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.h1,{children:"Discussion"}),"\n",(0,n.jsx)(t.p,{children:"The theoretical exploration of sound signals, music and the use of modern technologies, highlighted the complexity behind the interpretation we as users-listeners give to the sound experience. Furthermore, how the performance needs of interpretation change and are transformed according to the culture and the correlation between the medium and the listener. The study also highlighted how the attribution of meaning and significance to sound experience is a multifactorial matrix that operates in the form of language and evolves alongside social constructs."}),"\n",(0,n.jsx)(t.p,{children:"The implementation and successful results of the DL model confirm that extracting features from music and sound signals is feasible. Furthermore, the DL model confirms that the user-listener's sound perception can also be modelled in an AI environment with high accuracy. Even if the primary goal of VC generation was not achieved, certainly the prediction of musical genus is a key foundation for subsequent research in this direction."}),"\n",(0,n.jsx)(t.p,{children:"An important finding is the introduction of the ultrafrequency spectrum in the context of sensory perception of sound experience. As I discussed in the body of the study, even if we cannot distinguish frequencies through hearing these frequencies as vibration affect us. These super frequencies while not sonically discernible by human senses, are discernible through a digital audio file. Therefore, I believe that further exploration in the field of DL and audio signals using super frequencies is worthy of further investigation."}),"\n",(0,n.jsx)(t.p,{children:"This research, even at the point it has reached due to the limitations it has encountered, is an addition to the community of DL using audio signals. An addition in terms of prototype software pipeline design and creation of a customized data set. The DL model produced through this study may well be exploited in many ways. For example it can create tags with the musical genre as caption in movies. Moreover it can be used in the context of music streaming service to automate the labeling of tags.The prediction process may well work in reverse. As the infrastructure of the dataset and an API has been created, an app can be implemented which generates lists according to the term entered by the user-listener."}),"\n",(0,n.jsx)(t.p,{children:"Through the study of the correlations between musical genera, it was found that implementations in DL models generate new musical genera as sub-genres that complement the interpretation of the sound experience. From this point, this research could continue its work and shift to the identification of new features through music. Just as the identification of tonality was presented in feature extraction, exploring the concept of tonality variation within a piece of music could lead us to extract features such as musical scale. Hence, musical scale can convey features of mental state such as sadness and joy."}),"\n",(0,n.jsx)(t.p,{children:"The dataset created while conducting this research has also collected texts with the description for each musical piece. A possible future research could focus on NLP techniques so as to extract characteristics related to the musical piece through the description. In addition, the dataset has collected other variables such as tag_list. This variable consists of a set of tags that are not related to the musical genre. The utilization of these variables could enrich each music track in the existing dataset with new attributes."}),"\n",(0,n.jsx)(t.p,{children:"I intend to turn MLVC into an open source software and make it freely and publicly available to the DL community. MLVC could provide a framework for DL model development by providing an integrated software pipeline solution. The software pipeline to a large extent automates the processes from the individual steps in a DL project architecture."})]})}e=i.hmd(e),(0,a.j)({pageNextRoute:"/docs/discussion",pageOpts:{filePath:"pages/docs/discussion.mdx",route:"/docs/discussion",frontMatter:{},pageMap:[{kind:"Meta",data:{index:{title:"Home",type:"page",display:"hidden",theme:{typesetting:"article",layout:"full",breadcrumb:!0,footer:!0,sidebar:!1,toc:!0,pagination:!1}},docs:{title:"Research",type:"page"},dataset:{title:"Dataset",type:"page",theme:{typesetting:"article",layout:"default",breadcrumb:!0,footer:!0,sidebar:!0,toc:!0,pagination:!0}},experiments:{title:"Experiments",type:"page"},technology:{title:"Technology",type:"menu",items:{pytorch:{title:"Pytorch",href:"https://pytorch.org/",newWindow:!0},torchaudio:{title:"torch audio",href:"https://pytorch.org/audio/stable/index.html",newWindow:!0}}}}},{kind:"MdxPage",name:"dataset",route:"/dataset"},{kind:"Folder",name:"docs",route:"/docs",children:[{kind:"Meta",data:{abstract:{title:"Abstract",theme:{typesetting:"article",layout:"full"}},"Research Objectives":{type:"separator",title:"Research Objectives"},"problem-definition":"Problem definition",limitations:"Limitations","Project Architecture":{type:"separator",title:"Project Architecture"},"dl-pipeline":"DL Pipeline","technology-stack":"Technology Stack","Collecting data":{type:"separator",title:"Data Collection"},scraping:"Scraping",dataset:"Dataset","Exploratory analysis":{type:"separator",title:"Exploratory data analysis"},"data-exploration":"Audio Analysis","feature-extraction":"Feature Extraction","pre-processing":"Pre-processing",convolutional_network:{type:"separator",title:"Model"},cnn:"Convolutional Neural Network (CNN)",architecture:"Architecture",training:"Training",performance:"Performance",Conclusion:{type:"separator",title:"Conclusions"},contribution:"Contribution",discussion:"Discussion",endnotes:{type:"separator",title:"Endnotes"},sources:"Sources",references:"References","audio-datasets":"Audio Datasets"}},{kind:"MdxPage",name:"abstract",route:"/docs/abstract"},{kind:"MdxPage",name:"architecture",route:"/docs/architecture"},{kind:"MdxPage",name:"audio-datasets",route:"/docs/audio-datasets"},{kind:"MdxPage",name:"cnn",route:"/docs/cnn"},{kind:"MdxPage",name:"contribution",route:"/docs/contribution"},{kind:"MdxPage",name:"data-exploration",route:"/docs/data-exploration"},{kind:"Folder",name:"dataset",route:"/docs/dataset",children:[{kind:"Meta",data:{"design-principles":"Design Principles",taxonomy:"Taxonomy Construction","descriptive-statistics":"Descriptive Statistics"}},{kind:"MdxPage",name:"descriptive-statistics",route:"/docs/dataset/descriptive-statistics"},{kind:"MdxPage",name:"design-principles",route:"/docs/dataset/design-principles"},{kind:"MdxPage",name:"taxonomy",route:"/docs/dataset/taxonomy"}]},{kind:"MdxPage",name:"discussion",route:"/docs/discussion"},{kind:"MdxPage",name:"dl-pipeline",route:"/docs/dl-pipeline"},{kind:"MdxPage",name:"feature-extraction",route:"/docs/feature-extraction"},{kind:"MdxPage",name:"limitations",route:"/docs/limitations"},{kind:"MdxPage",name:"performance",route:"/docs/performance"},{kind:"MdxPage",name:"pre-processing",route:"/docs/pre-processing"},{kind:"MdxPage",name:"problem-definition",route:"/docs/problem-definition"},{kind:"MdxPage",name:"references",route:"/docs/references"},{kind:"Folder",name:"scraping",route:"/docs/scraping",children:[{kind:"Meta",data:{"streaming-services":"Streaming Service APIs","data-quality":"Quality of data","store-data":"Store Data",transformation:"Transformation & Cleaning"}},{kind:"MdxPage",name:"data-quality",route:"/docs/scraping/data-quality"},{kind:"MdxPage",name:"store-data",route:"/docs/scraping/store-data"},{kind:"MdxPage",name:"streaming-services",route:"/docs/scraping/streaming-services"},{kind:"MdxPage",name:"transformation",route:"/docs/scraping/transformation"}]},{kind:"MdxPage",name:"scraping",route:"/docs/scraping"},{kind:"MdxPage",name:"sources",route:"/docs/sources"},{kind:"MdxPage",name:"technology-stack",route:"/docs/technology-stack"},{kind:"MdxPage",name:"training",route:"/docs/training"}]},{kind:"Folder",name:"experiments",route:"/experiments",children:[{kind:"Meta",data:{Experiments:{type:"separator",title:"Experiments"},archive:"Archive",binary:"Binary Classification"}},{kind:"MdxPage",name:"archive",route:"/experiments/archive"},{kind:"Folder",name:"binary",route:"/experiments/binary",children:[{kind:"Meta",data:{Training:{type:"separator",title:"Training"},"experiment-1":"Experiment 1","experiment-2":"Experiment 2","experiment-3":"Experiment 3","experiment-4":"Experiment 4","experiment-5":"Experiment 5"}},{kind:"MdxPage",name:"experiment-1",route:"/experiments/binary/experiment-1"},{kind:"MdxPage",name:"experiment-2",route:"/experiments/binary/experiment-2"},{kind:"MdxPage",name:"experiment-3",route:"/experiments/binary/experiment-3"},{kind:"MdxPage",name:"experiment-4",route:"/experiments/binary/experiment-4"},{kind:"MdxPage",name:"experiment-5",route:"/experiments/binary/experiment-5"}]},{kind:"MdxPage",name:"binary",route:"/experiments/binary"}]},{kind:"MdxPage",name:"index",route:"/"}],headings:[{depth:1,value:"Discussion",id:"discussion"}],timestamp:167870459e4,flexsearch:{codeblocks:!0},title:"Discussion"},nextraLayout:o.ZP,themeConfig:s.Z,Content:function(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:t}=Object.assign({},(0,r.ah)(),e.components);return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(c,{...e})}):c(e)},hot:e.hot,pageOptsChecksum:"00000005ad3029a",dynamicMetaModules:[]})}},function(e){e.O(0,[774,32,888,179],function(){return e(e.s=1834)}),_N_E=e.O()}]);