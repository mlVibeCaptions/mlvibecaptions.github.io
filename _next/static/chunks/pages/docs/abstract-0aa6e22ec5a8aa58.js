(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[705],{601:function(e,t,i){(window.__NEXT_P=window.__NEXT_P||[]).push(["/docs/abstract",function(){return i(3451)}])},8177:function(e,t,i){"use strict";var n=i(5893);t.Z={logo:(0,n.jsx)("svg",{xmlns:"http://www.w3.org/2000/svg",width:"130",height:"30.555",viewBox:"0 0 130 30.555",children:(0,n.jsx)("g",{id:"Group_1","data-name":"Group 1",transform:"translate(-121.7 -308.1)",children:(0,n.jsx)("path",{id:"Union_1","data-name":"Union 1",d:"M95.7,15.435C95.7,6.4,102.979,0,113.224,0c8.6,0,15.028,4.124,16.776,10.722l-9.673,2c-.785-3.3-3.213-5.067-7.1-5.067-4.711,0-7.6,2.946-7.6,7.619,0,4.713,2.855,7.58,7.638,7.58,3.855,0,6.389-1.767,7.068-5.066l9.6,2.357c-2.106,6.6-8.353,10.408-17.026,10.408C102.408,30.555,95.7,24.664,95.7,15.435ZM73.959,29.613,61.5.982H71.889l6.818,17.6a13.878,13.878,0,0,1,.856,3.064h.143a19.312,19.312,0,0,1,.857-3.064L87.381.982H97.768L85.31,29.613Zm-43.3,0V13.9a41.259,41.259,0,0,1,.5-6.048h-.179c-.214,1.021-1.071,3.77-1.749,5.695l-5.64,16.063H14.885L9.566,13.628c-.714-2.239-1.321-4.4-1.642-5.773H7.781A37.945,37.945,0,0,1,8.138,13.9v15.71H0V.982H13.885l4.64,14.531a16.675,16.675,0,0,1,.964,3.574h.072a21.442,21.442,0,0,1,.964-3.653L24.986.982H39.228V29.613Zm11.458-.039V.982h9.388v20.7H67.891v7.894Z",transform:"translate(121.7 308.1)",fill:"#fff"})})}),project:{link:"https://github.com/mlVibeCaptions"},footer:{text:"MLVC 2023 - This website is an online documentation of Antonis kalagkatsis' MSc thesis in the National and Kapodistrian University of Athens, Department of Communication and Media Studies, Digital Communication Media and Interaction Environments"},primaryHue:189,feedback:{content:null},editLink:{text:null},sidebar:{defaultMenuCollapseLevel:1},useNextSeoProps:()=>({titleTemplate:"%s â€¢ MLVC"})}},3451:function(e,t,i){"use strict";i.r(t),i.d(t,{default:function(){return c.Z}});var n=i(5893),a=i(4526),o=i(7928),r=i(8177);i(5513);var s=i(1151);i(5675);var c=i(6092);function d(e){let t=Object.assign({h1:"h1",p:"p"},(0,s.ah)(),e.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.h1,{children:"Abstract"}),"\n",(0,n.jsx)(t.p,{children:"In today's world, the use of machine learning models is increasingly being incorporated into technological applications that we use on a daily basis. As technology incorporates machine learning algorithms to automate prediction and decision making by mimicking human operations, shifts in the ways in which we attach meaning and significance to the information we consume as users or receivers of it are created."}),"\n",(0,n.jsx)(t.p,{children:"This study, focuses on the information meaning-making of sound experience and music. It explores the notion of the label as an interface through the lens of contemporary music streaming services, while also exploring the shift of this semantics from musical genre to mood. It examines the impact that services incorporating machine learning algorithms have on the semantics of tagging as identity in music and sound experience. It studies the notion of musical gender as a label and its gradual transformation into a watery concept such as the description of the state of mental mood. At the same time, it analyses in depth the ways in which we evaluate sound experience and how the concept of auditory perception is formed by humans. This research aspires to introduce a new identity of signification of sound experience which is the Vibe Caption as an interface that describes the possible emotional state or atmosphere of a place, as it could be transmitted to and felt by others."}),"\n",(0,n.jsx)(t.p,{children:"The solution proposed through this study is the creation of an audio classifier using Deep learning algorithms and the use of convolutional neural networks. It is confirmed that the use of convolutional neural networks can predict features in music like human auditory perception.  The contribution of the research focuses on the use of convolutional networks to extract features from within the audio signal. In the context of developing the necessary software, the research contributes an integrated software pipeline solution for the repeated execution of machine learning experiments using artificial neural networks. The development methodology focuses on the continuous code integration software design technique and aspires to be published for free use as open source software. Due to major logistical constraints, to carry out this study, the methodological part managed to implement a sound classifier that identifies the musical genre as a prediction class with 95% accuracy on the training dataset."}),"\n",(0,n.jsx)(t.p,{children:"Keywords: Convolutional neural network, Mel spectrogram, Audio classifier, Machine learning, Artificial intelligence, Digital audio signal processing, Deep learning, Convolutional neural network, Audio signal processing, Audio classifier"})]})}e=i.hmd(e),(0,a.j)({pageNextRoute:"/docs/abstract",pageOpts:{filePath:"pages/docs/abstract.mdx",route:"/docs/abstract",frontMatter:{},pageMap:[{kind:"Meta",data:{index:{title:"Home",type:"page",display:"hidden",theme:{typesetting:"article",layout:"full",breadcrumb:!0,footer:!0,sidebar:!1,toc:!0,pagination:!1}},docs:{title:"Research",type:"page"},dataset:{title:"Dataset",type:"page",theme:{typesetting:"article",layout:"default",breadcrumb:!0,footer:!0,sidebar:!0,toc:!0,pagination:!0}},experiments:{title:"Experiments",type:"page"},technology:{title:"Technology",type:"menu",items:{pytorch:{title:"Pytorch",href:"https://pytorch.org/",newWindow:!0},torchaudio:{title:"torch audio",href:"https://pytorch.org/audio/stable/index.html",newWindow:!0}}}}},{kind:"MdxPage",name:"dataset",route:"/dataset"},{kind:"Folder",name:"docs",route:"/docs",children:[{kind:"Meta",data:{abstract:{title:"Abstract",theme:{typesetting:"article",layout:"full"}},"Research Objectives":{type:"separator",title:"Research Objectives"},"problem-definition":"Problem definition",limitations:"Limitations","Project Architecture":{type:"separator",title:"Project Architecture"},"dl-pipeline":"DL Pipeline","technology-stack":"Technology Stack","Collecting data":{type:"separator",title:"Data Collection"},scraping:"Scraping",dataset:"Dataset","Exploratory analysis":{type:"separator",title:"Exploratory data analysis"},"data-exploration":"Audio Analysis","feature-extraction":"Feature Extraction","pre-processing":"Pre-processing",convolutional_network:{type:"separator",title:"Model"},cnn:"Convolutional Neural Network (CNN)",architecture:"Architecture",training:"Training",performance:"Performance",Conclusion:{type:"separator",title:"Conclusions"},contribution:"Contribution",discussion:"Discussion",endnotes:{type:"separator",title:"Endnotes"},sources:"Sources",references:"References","audio-datasets":"Audio Datasets"}},{kind:"MdxPage",name:"abstract",route:"/docs/abstract"},{kind:"MdxPage",name:"architecture",route:"/docs/architecture"},{kind:"MdxPage",name:"audio-datasets",route:"/docs/audio-datasets"},{kind:"MdxPage",name:"cnn",route:"/docs/cnn"},{kind:"MdxPage",name:"contribution",route:"/docs/contribution"},{kind:"MdxPage",name:"data-exploration",route:"/docs/data-exploration"},{kind:"Folder",name:"dataset",route:"/docs/dataset",children:[{kind:"Meta",data:{"design-principles":"Design Principles",taxonomy:"Taxonomy Construction","descriptive-statistics":"Descriptive Statistics"}},{kind:"MdxPage",name:"descriptive-statistics",route:"/docs/dataset/descriptive-statistics"},{kind:"MdxPage",name:"design-principles",route:"/docs/dataset/design-principles"},{kind:"MdxPage",name:"taxonomy",route:"/docs/dataset/taxonomy"}]},{kind:"MdxPage",name:"discussion",route:"/docs/discussion"},{kind:"MdxPage",name:"dl-pipeline",route:"/docs/dl-pipeline"},{kind:"MdxPage",name:"feature-extraction",route:"/docs/feature-extraction"},{kind:"MdxPage",name:"limitations",route:"/docs/limitations"},{kind:"MdxPage",name:"performance",route:"/docs/performance"},{kind:"MdxPage",name:"pre-processing",route:"/docs/pre-processing"},{kind:"MdxPage",name:"problem-definition",route:"/docs/problem-definition"},{kind:"MdxPage",name:"references",route:"/docs/references"},{kind:"Folder",name:"scraping",route:"/docs/scraping",children:[{kind:"Meta",data:{"streaming-services":"Streaming Service APIs","data-quality":"Quality of data","store-data":"Store Data",transformation:"Transformation & Cleaning"}},{kind:"MdxPage",name:"data-quality",route:"/docs/scraping/data-quality"},{kind:"MdxPage",name:"store-data",route:"/docs/scraping/store-data"},{kind:"MdxPage",name:"streaming-services",route:"/docs/scraping/streaming-services"},{kind:"MdxPage",name:"transformation",route:"/docs/scraping/transformation"}]},{kind:"MdxPage",name:"scraping",route:"/docs/scraping"},{kind:"MdxPage",name:"sources",route:"/docs/sources"},{kind:"MdxPage",name:"technology-stack",route:"/docs/technology-stack"},{kind:"MdxPage",name:"training",route:"/docs/training"}]},{kind:"Folder",name:"experiments",route:"/experiments",children:[{kind:"Meta",data:{Experiments:{type:"separator",title:"Experiments"},archive:"Archive",binary:"Binary Classification"}},{kind:"MdxPage",name:"archive",route:"/experiments/archive"},{kind:"Folder",name:"binary",route:"/experiments/binary",children:[{kind:"Meta",data:{Training:{type:"separator",title:"Training"},"experiment-1":"Experiment 1","experiment-2":"Experiment 2","experiment-3":"Experiment 3","experiment-4":"Experiment 4","experiment-5":"Experiment 5"}},{kind:"MdxPage",name:"experiment-1",route:"/experiments/binary/experiment-1"},{kind:"MdxPage",name:"experiment-2",route:"/experiments/binary/experiment-2"},{kind:"MdxPage",name:"experiment-3",route:"/experiments/binary/experiment-3"},{kind:"MdxPage",name:"experiment-4",route:"/experiments/binary/experiment-4"},{kind:"MdxPage",name:"experiment-5",route:"/experiments/binary/experiment-5"}]},{kind:"MdxPage",name:"binary",route:"/experiments/binary"}]},{kind:"MdxPage",name:"index",route:"/"}],headings:[{depth:1,value:"Abstract",id:"abstract"}],timestamp:167870459e4,flexsearch:{codeblocks:!0},title:"Abstract"},nextraLayout:o.ZP,themeConfig:r.Z,Content:function(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:t}=Object.assign({},(0,s.ah)(),e.components);return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(d,{...e})}):d(e)},hot:e.hot,pageOptsChecksum:"0000000fe189634",dynamicMetaModules:[]})}},function(e){e.O(0,[774,32,888,179],function(){return e(e.s=601)}),_N_E=e.O()}]);