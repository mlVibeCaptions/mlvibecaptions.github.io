(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[416],{6919:function(e,t,n){(window.__NEXT_P=window.__NEXT_P||[]).push(["/docs/contribution",function(){return n(6076)}])},8177:function(e,t,n){"use strict";var a=n(5893);t.Z={logo:(0,a.jsx)("svg",{xmlns:"http://www.w3.org/2000/svg",width:"130",height:"30.555",viewBox:"0 0 130 30.555",children:(0,a.jsx)("g",{id:"Group_1","data-name":"Group 1",transform:"translate(-121.7 -308.1)",children:(0,a.jsx)("path",{id:"Union_1","data-name":"Union 1",d:"M95.7,15.435C95.7,6.4,102.979,0,113.224,0c8.6,0,15.028,4.124,16.776,10.722l-9.673,2c-.785-3.3-3.213-5.067-7.1-5.067-4.711,0-7.6,2.946-7.6,7.619,0,4.713,2.855,7.58,7.638,7.58,3.855,0,6.389-1.767,7.068-5.066l9.6,2.357c-2.106,6.6-8.353,10.408-17.026,10.408C102.408,30.555,95.7,24.664,95.7,15.435ZM73.959,29.613,61.5.982H71.889l6.818,17.6a13.878,13.878,0,0,1,.856,3.064h.143a19.312,19.312,0,0,1,.857-3.064L87.381.982H97.768L85.31,29.613Zm-43.3,0V13.9a41.259,41.259,0,0,1,.5-6.048h-.179c-.214,1.021-1.071,3.77-1.749,5.695l-5.64,16.063H14.885L9.566,13.628c-.714-2.239-1.321-4.4-1.642-5.773H7.781A37.945,37.945,0,0,1,8.138,13.9v15.71H0V.982H13.885l4.64,14.531a16.675,16.675,0,0,1,.964,3.574h.072a21.442,21.442,0,0,1,.964-3.653L24.986.982H39.228V29.613Zm11.458-.039V.982h9.388v20.7H67.891v7.894Z",transform:"translate(121.7 308.1)",fill:"#fff"})})}),project:{link:"https://github.com/mlVibeCaptions"},footer:{text:"MLVC 2023 - This website is an online documentation of Antonis kalagkatsis' MSc thesis in the National and Kapodistrian University of Athens, Department of Communication and Media Studies, Digital Communication Media and Interaction Environments"},primaryHue:189,feedback:{content:null},editLink:{text:null},sidebar:{defaultMenuCollapseLevel:1},useNextSeoProps:()=>({titleTemplate:"%s â€¢ MLVC"})}},6076:function(e,t,n){"use strict";n.r(t),n.d(t,{default:function(){return c.Z}});var a=n(5893),i=n(4526),r=n(7928),o=n(8177);n(5513);var s=n(1151);n(5675);var c=n(6092);function d(e){let t=Object.assign({h1:"h1",p:"p"},(0,s.ah)(),e.components);return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.h1,{children:"Contribution"}),"\n",(0,a.jsx)(t.p,{children:"As the survey showed, users are not satisfied with the classification of music by genre. The fact that even the services leave the possibility of assigning meaning to the tag interface free for users, confirms how the musical genre is slowly starting to disappear as the dominant term of interpretation and classification in music. It confirms the trend, and especially in Gen-Z, that user-listeners are beginning to interpret music in sensory ways and to assign meaning such that it projects a state of mind."}),"\n",(0,a.jsx)(t.p,{children:"In the phase of analyzing the genealogy of the musical genus, the research identified examples where even technological AI software has established new musical genera as sub-genera. These sub-genera come to cover mental state characteristics while acting as extensions to existing traditional musical genera. In addition, these sub-genres carry features of seasonality and localization, such as for example LoFi House which was created by YouTube's DL algorithm as a playlist. Hence, this artificially created sub-genus can on the one hand describe aesthetic features in music and on the other hand increase the semantics of the main genus from which it originated. Therefore, these sub-genres function as a miniature of the term vibe that this study investigates."}),"\n",(0,a.jsx)(t.p,{children:"The theoretical framing of the study, extended to the way we perceive the sound experience in sensory terms. It analysed the concept of sound as vibration, and how it interacts with our body either through hearing or through the environment as the dominant mediator of the sound event. There was extensive analysis around the formation of human auditory perception, and the approaches by which we distinguish features in a sound experience or musical event. This work even extended to the effect of sound vibrations through the frequency spectrum that is not perceptible to human hearing. In addition, it examined applications of sound signals used as crowd control and crowd suppression tools."}),"\n",(0,a.jsx)(t.p,{children:"This research confirmed that whether we refer to music or a sound event, our interpretation of a sound experience is multifactorial. Factors such as memory referring to familiarity, space as a modulator of the sound signal, place as a geographically defined memory, and content which we evaluate through the dual listening approach (compositional and sensory) come together to compose the overall aesthetics behind a sound event. Hence, the features that shape the aesthetic experience in a sound event are not only musical."}),"\n",(0,a.jsx)(t.p,{children:"In the following, this study implements a machine learning model to predict musical genre. This implementation is a small part of the larger problem which is the generation of vibe captions. After a thorough research around implementations of DL models using digital audio signals, it was found how there are many examples that can detect human speech. As, there are many speech recognition applications today of which we currently use on a daily basis. It was also found that there are several researches on the part of sound recognition of sounds within the urban landscape or environment. These researches were aimed at implementing models that can project events through sound events for safety in workplaces. In music it was also confirmed that there are researches dealing with information extraction through music (MIR) and classification in music using DL models."}),"\n",(0,a.jsx)(t.p,{children:"This research through the implementation of ACLF has demonstrated how the prediction of a musical genus using DL algorithms and CNN is feasible and with very high accuracy. The DL model created from this study can distinguish the musical genre jazz and grime with an accuracy of 95%. Which confirms the value of continuing this research in order to reach the big goal set, the real-time VC production."})]})}e=n.hmd(e),(0,i.j)({pageNextRoute:"/docs/contribution",pageOpts:{filePath:"pages/docs/contribution.mdx",route:"/docs/contribution",frontMatter:{},pageMap:[{kind:"Meta",data:{index:{title:"Home",type:"page",display:"hidden",theme:{typesetting:"article",layout:"full",breadcrumb:!0,footer:!0,sidebar:!1,toc:!0,pagination:!1}},docs:{title:"Research",type:"page"},dataset:{title:"Dataset",type:"page",theme:{typesetting:"article",layout:"default",breadcrumb:!0,footer:!0,sidebar:!0,toc:!0,pagination:!0}},experiments:{title:"Experiments",type:"page"},technology:{title:"Technology",type:"menu",items:{pytorch:{title:"Pytorch",href:"https://pytorch.org/",newWindow:!0},torchaudio:{title:"torch audio",href:"https://pytorch.org/audio/stable/index.html",newWindow:!0}}}}},{kind:"MdxPage",name:"dataset",route:"/dataset"},{kind:"Folder",name:"docs",route:"/docs",children:[{kind:"Meta",data:{abstract:{title:"Abstract",theme:{typesetting:"article",layout:"full"}},"Research Objectives":{type:"separator",title:"Research Objectives"},"problem-definition":"Problem definition",limitations:"Limitations","Project Architecture":{type:"separator",title:"Project Architecture"},"dl-pipeline":"DL Pipeline","technology-stack":"Technology Stack","Collecting data":{type:"separator",title:"Data Collection"},scraping:"Scraping",dataset:"Dataset","Exploratory analysis":{type:"separator",title:"Exploratory data analysis"},"data-exploration":"Audio Analysis","feature-extraction":"Feature Extraction","pre-processing":"Pre-processing",convolutional_network:{type:"separator",title:"Model"},cnn:"Convolutional Neural Network (CNN)",architecture:"Architecture",training:"Training",performance:"Performance",Conclusion:{type:"separator",title:"Conclusions"},contribution:"Contribution",discussion:"Discussion",endnotes:{type:"separator",title:"Endnotes"},sources:"Sources",references:"References","audio-datasets":"Audio Datasets"}},{kind:"MdxPage",name:"abstract",route:"/docs/abstract"},{kind:"MdxPage",name:"architecture",route:"/docs/architecture"},{kind:"MdxPage",name:"audio-datasets",route:"/docs/audio-datasets"},{kind:"MdxPage",name:"cnn",route:"/docs/cnn"},{kind:"MdxPage",name:"contribution",route:"/docs/contribution"},{kind:"MdxPage",name:"data-exploration",route:"/docs/data-exploration"},{kind:"Folder",name:"dataset",route:"/docs/dataset",children:[{kind:"Meta",data:{"design-principles":"Design Principles",taxonomy:"Taxonomy Construction","descriptive-statistics":"Descriptive Statistics"}},{kind:"MdxPage",name:"descriptive-statistics",route:"/docs/dataset/descriptive-statistics"},{kind:"MdxPage",name:"design-principles",route:"/docs/dataset/design-principles"},{kind:"MdxPage",name:"taxonomy",route:"/docs/dataset/taxonomy"}]},{kind:"MdxPage",name:"discussion",route:"/docs/discussion"},{kind:"MdxPage",name:"dl-pipeline",route:"/docs/dl-pipeline"},{kind:"MdxPage",name:"feature-extraction",route:"/docs/feature-extraction"},{kind:"MdxPage",name:"limitations",route:"/docs/limitations"},{kind:"MdxPage",name:"performance",route:"/docs/performance"},{kind:"MdxPage",name:"pre-processing",route:"/docs/pre-processing"},{kind:"MdxPage",name:"problem-definition",route:"/docs/problem-definition"},{kind:"MdxPage",name:"references",route:"/docs/references"},{kind:"Folder",name:"scraping",route:"/docs/scraping",children:[{kind:"Meta",data:{"streaming-services":"Streaming Service APIs","data-quality":"Quality of data","store-data":"Store Data",transformation:"Transformation & Cleaning"}},{kind:"MdxPage",name:"data-quality",route:"/docs/scraping/data-quality"},{kind:"MdxPage",name:"store-data",route:"/docs/scraping/store-data"},{kind:"MdxPage",name:"streaming-services",route:"/docs/scraping/streaming-services"},{kind:"MdxPage",name:"transformation",route:"/docs/scraping/transformation"}]},{kind:"MdxPage",name:"scraping",route:"/docs/scraping"},{kind:"MdxPage",name:"sources",route:"/docs/sources"},{kind:"MdxPage",name:"technology-stack",route:"/docs/technology-stack"},{kind:"MdxPage",name:"training",route:"/docs/training"}]},{kind:"Folder",name:"experiments",route:"/experiments",children:[{kind:"Meta",data:{Experiments:{type:"separator",title:"Experiments"},archive:"Archive",binary:"Binary Classification"}},{kind:"MdxPage",name:"archive",route:"/experiments/archive"},{kind:"Folder",name:"binary",route:"/experiments/binary",children:[{kind:"Meta",data:{Training:{type:"separator",title:"Training"},"experiment-1":"Experiment 1","experiment-2":"Experiment 2","experiment-3":"Experiment 3","experiment-4":"Experiment 4","experiment-5":"Experiment 5"}},{kind:"MdxPage",name:"experiment-1",route:"/experiments/binary/experiment-1"},{kind:"MdxPage",name:"experiment-2",route:"/experiments/binary/experiment-2"},{kind:"MdxPage",name:"experiment-3",route:"/experiments/binary/experiment-3"},{kind:"MdxPage",name:"experiment-4",route:"/experiments/binary/experiment-4"},{kind:"MdxPage",name:"experiment-5",route:"/experiments/binary/experiment-5"}]},{kind:"MdxPage",name:"binary",route:"/experiments/binary"}]},{kind:"MdxPage",name:"index",route:"/"}],headings:[{depth:1,value:"Contribution",id:"contribution"}],timestamp:167870459e4,flexsearch:{codeblocks:!0},title:"Contribution"},nextraLayout:r.ZP,themeConfig:o.Z,Content:function(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:t}=Object.assign({},(0,s.ah)(),e.components);return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(d,{...e})}):d(e)},hot:e.hot,pageOptsChecksum:"000000098717f7c",dynamicMetaModules:[]})}},function(e){e.O(0,[774,32,888,179],function(){return e(e.s=6919)}),_N_E=e.O()}]);