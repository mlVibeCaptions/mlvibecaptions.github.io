(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[239],{9777:function(e,t,i){(window.__NEXT_P=window.__NEXT_P||[]).push(["/docs/problem-definition",function(){return i(4690)}])},8177:function(e,t,i){"use strict";var n=i(5893);t.Z={logo:(0,n.jsx)("svg",{xmlns:"http://www.w3.org/2000/svg",width:"130",height:"30.555",viewBox:"0 0 130 30.555",children:(0,n.jsx)("g",{id:"Group_1","data-name":"Group 1",transform:"translate(-121.7 -308.1)",children:(0,n.jsx)("path",{id:"Union_1","data-name":"Union 1",d:"M95.7,15.435C95.7,6.4,102.979,0,113.224,0c8.6,0,15.028,4.124,16.776,10.722l-9.673,2c-.785-3.3-3.213-5.067-7.1-5.067-4.711,0-7.6,2.946-7.6,7.619,0,4.713,2.855,7.58,7.638,7.58,3.855,0,6.389-1.767,7.068-5.066l9.6,2.357c-2.106,6.6-8.353,10.408-17.026,10.408C102.408,30.555,95.7,24.664,95.7,15.435ZM73.959,29.613,61.5.982H71.889l6.818,17.6a13.878,13.878,0,0,1,.856,3.064h.143a19.312,19.312,0,0,1,.857-3.064L87.381.982H97.768L85.31,29.613Zm-43.3,0V13.9a41.259,41.259,0,0,1,.5-6.048h-.179c-.214,1.021-1.071,3.77-1.749,5.695l-5.64,16.063H14.885L9.566,13.628c-.714-2.239-1.321-4.4-1.642-5.773H7.781A37.945,37.945,0,0,1,8.138,13.9v15.71H0V.982H13.885l4.64,14.531a16.675,16.675,0,0,1,.964,3.574h.072a21.442,21.442,0,0,1,.964-3.653L24.986.982H39.228V29.613Zm11.458-.039V.982h9.388v20.7H67.891v7.894Z",transform:"translate(121.7 308.1)",fill:"#fff"})})}),project:{link:"https://github.com/mlVibeCaptions"},footer:{text:"MLVC 2023 - This website is an online documentation of Antonis kalagkatsis' MSc thesis in the National and Kapodistrian University of Athens, Department of Communication and Media Studies, Digital Communication Media and Interaction Environments"},primaryHue:189,feedback:{content:null},editLink:{text:null},sidebar:{defaultMenuCollapseLevel:1},useNextSeoProps:()=>({titleTemplate:"%s â€¢ MLVC"})}},4690:function(e,t,i){"use strict";i.r(t),i.d(t,{default:function(){return d.Z}});var n=i(5893),a=i(4526),o=i(7928),r=i(8177);i(5513);var s=i(1151);i(5675);var d=i(6092);function c(e){let t=Object.assign({h1:"h1",p:"p",h2:"h2"},(0,s.ah)(),e.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.h1,{children:"Problem definition"}),"\n",(0,n.jsx)(t.p,{children:"Since the theoretical framework of the research was identified in the previous chapters, in this chapter I discuss the implementation process of the audio classifier (ACLF) using CNN architectures. The analysis will be done in stages, exactly in the chronological order and organization that the development was done in the software. Therefore, the methodological analysis is divided into four sections. Data collection, dataset design, data exploration and finally the design of the classification model. From now on, the implemented software in the context of this research will be referred to as machine learning vibe captions (MLVC) and then for each application that supports the overall implementation of it, the corresponding suffix will be added. We will see this later on, you can refer to the glossary for more information.\nDefining Objectives\nPerhaps the most difficult part of designing software that uses DL algorithms and ANNs is formulating the goals. That is, what is the outcome we want to predict and what types of data we need to explore. Obviously, the answer to these questions determines the definition of the problem that we will see next. At the same time, however, it is the objective that will determine the methodology in the development and implementation phase of the DL model."}),"\n",(0,n.jsx)(t.p,{children:"This research has set the goal already from the beginning which is to predict and recognize the emotional state through listening to an audio experience, formulated as a vibe capture (VC) in real time. As already defined by the theoretical framework, the concept of VC refers to a synthesis of a description of the sound with elements from the feature set in musical terms, details around the way of music composition, emotion, mood, memory, place. That is, during the process of listening to music we ideally want the model we develop to be able to predict in real time the above features and formulate them as a description. To be more successful in illustrating the VC concept I will use the standard model in an example form, formulated in three versions that unfold in time and are generated in real time below.\nPrototypical examples of VC generation are the 3 below."}),"\n",(0,n.jsx)(t.p,{children:"VC1. sails between ambient and ethereal music"}),"\n",(0,n.jsx)(t.p,{children:"VC2. inspired by soft reverbed vocals and melodious chord progressions"}),"\n",(0,n.jsx)(t.p,{children:"VC3. immersive nostalgic journey in a sea of synthetic strings and choirs."}),"\n",(0,n.jsx)(t.h2,{id:"define-objectives",children:"Define objectives"}),"\n",(0,n.jsx)(t.p,{children:"Once the objective has been defined, the next step is to formulate and define the problem. The problem invoked in this research is complex, as the prediction or goal we want to produce through DL is not a predefined class or a predefined label.\nThe output data of the model is therefore a sentence that on the one hand should follow the rules of syntax and grammar, and on the other hand should accurately convey meaning and significance as interpretive features resulting from our interaction with sound experience and music. These features should be generated through ACLF. A possible first interpretation of the problem can be formulated as multiclass multilabel multitask classification. That is, a classification problem with multiple input variables, multiple labels and multiple processes."}),"\n",(0,n.jsx)(t.p,{children:"The main difference between multiclass classification and multitask classification is the number of outcome variables in the model.\nIn multi-category classification there is only one outcome variable, whereas in multi-task classification there are many outcome variables that must be considered together. However, the output variables are also directly related to the input dataset and its design. We will address this issue in the next section which is data collection and dataset design for this research. Hence at this point the research undertakes a process of fragmentation of the larger problem into smaller problems."})]})}e=i.hmd(e),(0,a.j)({pageNextRoute:"/docs/problem-definition",pageOpts:{filePath:"pages/docs/problem-definition.mdx",route:"/docs/problem-definition",frontMatter:{},pageMap:[{kind:"Meta",data:{index:{title:"Home",type:"page",display:"hidden",theme:{typesetting:"article",layout:"full",breadcrumb:!0,footer:!0,sidebar:!1,toc:!0,pagination:!1}},docs:{title:"Research",type:"page"},dataset:{title:"Dataset",type:"page",theme:{typesetting:"article",layout:"default",breadcrumb:!0,footer:!0,sidebar:!0,toc:!0,pagination:!0}},experiments:{title:"Experiments",type:"page"},technology:{title:"Technology",type:"menu",items:{pytorch:{title:"Pytorch",href:"https://pytorch.org/",newWindow:!0},torchaudio:{title:"torch audio",href:"https://pytorch.org/audio/stable/index.html",newWindow:!0}}}}},{kind:"MdxPage",name:"dataset",route:"/dataset"},{kind:"Folder",name:"docs",route:"/docs",children:[{kind:"Meta",data:{abstract:{title:"Abstract",theme:{typesetting:"article",layout:"full"}},"Research Objectives":{type:"separator",title:"Research Objectives"},"problem-definition":"Problem definition",limitations:"Limitations","Project Architecture":{type:"separator",title:"Project Architecture"},"dl-pipeline":"DL Pipeline","technology-stack":"Technology Stack","Collecting data":{type:"separator",title:"Data Collection"},scraping:"Scraping",dataset:"Dataset","Exploratory analysis":{type:"separator",title:"Exploratory data analysis"},"data-exploration":"Audio Analysis","feature-extraction":"Feature Extraction","pre-processing":"Pre-processing",convolutional_network:{type:"separator",title:"Model"},cnn:"Convolutional Neural Network (CNN)",architecture:"Architecture",training:"Training",performance:"Performance",Conclusion:{type:"separator",title:"Conclusions"},contribution:"Contribution",discussion:"Discussion",endnotes:{type:"separator",title:"Endnotes"},sources:"Sources",references:"References","audio-datasets":"Audio Datasets"}},{kind:"MdxPage",name:"abstract",route:"/docs/abstract"},{kind:"MdxPage",name:"architecture",route:"/docs/architecture"},{kind:"MdxPage",name:"audio-datasets",route:"/docs/audio-datasets"},{kind:"MdxPage",name:"cnn",route:"/docs/cnn"},{kind:"MdxPage",name:"contribution",route:"/docs/contribution"},{kind:"MdxPage",name:"data-exploration",route:"/docs/data-exploration"},{kind:"Folder",name:"dataset",route:"/docs/dataset",children:[{kind:"Meta",data:{"design-principles":"Design Principles",taxonomy:"Taxonomy Construction","descriptive-statistics":"Descriptive Statistics"}},{kind:"MdxPage",name:"descriptive-statistics",route:"/docs/dataset/descriptive-statistics"},{kind:"MdxPage",name:"design-principles",route:"/docs/dataset/design-principles"},{kind:"MdxPage",name:"taxonomy",route:"/docs/dataset/taxonomy"}]},{kind:"MdxPage",name:"discussion",route:"/docs/discussion"},{kind:"MdxPage",name:"dl-pipeline",route:"/docs/dl-pipeline"},{kind:"MdxPage",name:"feature-extraction",route:"/docs/feature-extraction"},{kind:"MdxPage",name:"limitations",route:"/docs/limitations"},{kind:"MdxPage",name:"performance",route:"/docs/performance"},{kind:"MdxPage",name:"pre-processing",route:"/docs/pre-processing"},{kind:"MdxPage",name:"problem-definition",route:"/docs/problem-definition"},{kind:"MdxPage",name:"references",route:"/docs/references"},{kind:"Folder",name:"scraping",route:"/docs/scraping",children:[{kind:"Meta",data:{"streaming-services":"Streaming Service APIs","data-quality":"Quality of data","store-data":"Store Data",transformation:"Transformation & Cleaning"}},{kind:"MdxPage",name:"data-quality",route:"/docs/scraping/data-quality"},{kind:"MdxPage",name:"store-data",route:"/docs/scraping/store-data"},{kind:"MdxPage",name:"streaming-services",route:"/docs/scraping/streaming-services"},{kind:"MdxPage",name:"transformation",route:"/docs/scraping/transformation"}]},{kind:"MdxPage",name:"scraping",route:"/docs/scraping"},{kind:"MdxPage",name:"sources",route:"/docs/sources"},{kind:"MdxPage",name:"technology-stack",route:"/docs/technology-stack"},{kind:"MdxPage",name:"training",route:"/docs/training"}]},{kind:"Folder",name:"experiments",route:"/experiments",children:[{kind:"Meta",data:{Experiments:{type:"separator",title:"Experiments"},archive:"Archive",binary:"Binary Classification"}},{kind:"MdxPage",name:"archive",route:"/experiments/archive"},{kind:"Folder",name:"binary",route:"/experiments/binary",children:[{kind:"Meta",data:{Training:{type:"separator",title:"Training"},"experiment-1":"Experiment 1","experiment-2":"Experiment 2","experiment-3":"Experiment 3","experiment-4":"Experiment 4","experiment-5":"Experiment 5"}},{kind:"MdxPage",name:"experiment-1",route:"/experiments/binary/experiment-1"},{kind:"MdxPage",name:"experiment-2",route:"/experiments/binary/experiment-2"},{kind:"MdxPage",name:"experiment-3",route:"/experiments/binary/experiment-3"},{kind:"MdxPage",name:"experiment-4",route:"/experiments/binary/experiment-4"},{kind:"MdxPage",name:"experiment-5",route:"/experiments/binary/experiment-5"}]},{kind:"MdxPage",name:"binary",route:"/experiments/binary"}]},{kind:"MdxPage",name:"index",route:"/"}],headings:[{depth:1,value:"Problem definition",id:"problem-definition"},{depth:2,value:"Define objectives",id:"define-objectives"}],timestamp:167870459e4,flexsearch:{codeblocks:!0},title:"Problem definition"},nextraLayout:o.ZP,themeConfig:r.Z,Content:function(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:t}=Object.assign({},(0,s.ah)(),e.components);return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(c,{...e})}):c(e)},hot:e.hot,pageOptsChecksum:"00000008160afab",dynamicMetaModules:[]})}},function(e){e.O(0,[774,32,888,179],function(){return e(e.s=9777)}),_N_E=e.O()}]);